---
title: "An Exploratory Data Analysis of the CSZ and NSAF from 2011-2021"
author: "Swetha Natarajan, N.C State University"
output: 
  rmarkdown::html_document:
    theme: lumen 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Abstract

The Cascadia Subduction Zone (CSZ) is located from Northern Vancouver Island to Cape Mendocino, California. This region of intense tectonic activity is predicted to cause a magnitude 9.0 or greater earthquake that will wipe out most of the north-western seaboard of the United States. These earthquakes occur on inconsistent intervals that range from 200 to 1,000 years. Hence, scientists are unable to make confident predictions about tectonic activity in Cascadia.

The Northern San Andreas Fault is a region of frequent seismic activity and stretches from the Mendocino Triple Junction through the San Francisco Bay Area. Research suggestions that quakes in the CSZ could have triggered earthquakes in NSAF in the past one thousand years.

In order to progress research in this area, the following investigation of CSZ and NSAF provides an understanding of our approach to this eventual earthquake and enhanced predictions on its location, depth, and magnitude. This exploratory analysis of earthquake data from 2011-2021 in both regions will aid scientists in understanding patterns of seismic behavior in both regions and determine whether a causal relationship exists between the two faults.

### Background

Subduction zones are areas where two tectonic plates meet, allowing one plate to slide under the other and into the Earth's mantle. The Juan de Fuca plate dives under the North American Plate to create CSZ, as seen in Image 2.

![Image 2: Tectonic Plates of the CSZ. Graphic by the National Parks= Service](Image2.jpg){width=50%}

The slipping motion between both of these plates is the cause of "megathrust earthquakes", which have a magnitude of 8.5 or higher. Image 4 shows an aerial view of the side by side plates. On average, CSZ produces an earthquake every two hundred and forty-three years, the most recent being a magnitude 9.0 in January 1700. The area is overdue for an earthquake; we've coined this future earthquake "The Big One" to commemorate its size. If accurate predictions can be made using data from nearby faults and the past, then governments and residents can prepare accordingly.![Image 4: Aerial View of the CSZ. Graphic by UC Berkeley](Image4.jpg){width=200px}



However, due to inconsistent, lengthy intervals and a low occurrence rate, it is difficult to predict when the next “big one” will occur. Plate movement in this region is analyzed through current and past seismic activity in the area (increases in magnitude, occurrences, and changes in earthquake depth). It is speculated that a megathrust earthquake in Cascadia could trigger seismic activity in the San Andreas Fault, meaning movement in the San Andreas fault could be a result of activity in Cascadia. The effects of Cascadia could affect regions up to Sacramento, California, which lies on the Northern San Andreas Fault. My data collection and exploration includes earthquakes up to Sacramento for the possibility of future analysis of the relationship between the two faults.

## Introduction

The characteristics of depth, magnitude, and time can be used to draw conclusions about seismic activity. **Depth** is a measurement of the distance between the earth’s surface and the location at which the earthquake begins to rupture (U.S Geological Survey, 2022). This point where the earthquake ruptures is its epicenter. Earthquakes can occur either in the crust or upper mantle of the earth, approximately 800 kilometers into the Earth’s surface. 

**Magnitude** is characterized by the maximum motion recorded by a seismograph, and is regarded as the size of an earthquake (U.S Geological Survey, 2022). Different scales can be used to measure magnitude; the Richter scale or the Mercalli scale. Regardless of the scale used, measurements between the two are relatively the same. The Mercalli scale is more reliable for measuring larger earthquakes and the Richter scale is more reliable for smaller earthquakes. The USGS ComCat measurements of magnitude follow the Richter scale. This scale uses a logarithmic interval to determine the amount of energy released by an earthquake, and magnitudes range from approximately 1.0 to 9.0, with 9.0 being the highest. Since it follows a logarithmic scale, seismic waves from a seismographs for a 6.0 earthquake are ten times less than that of a 7.0 earthquake. 

**Intensity** of an earthquake defines the severity of the shaking produced by an earthquake. The intensity of an earthquake, regardless of magnitude, is dependent on its depth, as intensity decreases the greater the distance from the earthquake’s epicenter. An earthquake with an increased distance from the earth’s surface results in less shaking compared to an earthquake with a lower depth (U.S Geological Survey, 2022). Intensity is also often based on the effects of an earthquake on persons, structures, and the environment.. 

The relationship between depth, magnitude, and intensity is as follows; the shallower the earthquake and the larger the magnitude, the more intense it is. An 7.0 earthquake with a depth of 70 km will likely result in less damage compared to the same earthquake at a depth of 30 km under the surface. An increase in higher intensity earthquakes may indicate danger for CSZ. 

**Time** in this study is used to observe changes in seismic activity in and around CSZ and NSAF from 2011-2021. **Duration**, a measurement of time, can be used to indicate the type of earthquake observed. Duration can either be the amount of time it takes a fault to crack, or the amount of time shaking is felt on the surface; this variable is not included in our dataset. **Very Low-Frequency Earthquakes** (VLEs) are a new variety of earthquakes discovered in parts of Japan and around CSZ (Ide et al., 2007). The subduction zone has seen LFEs, or low-frequency earthquakes in its southern margin. LFEs are small earthquakes that occur along subduction zones with continuous tectonic tremors for longer periods of time. VLEs are a subset of LFEs with a frequency between 0.01– 0.10 Hz, while LFEs have a frequency  > 1 Hz. In Cascadia, LFEs have occurred for 2-3 week long periods every 10-20 months (Plourde et al., 2015). It’s likely our data has picked up on a slow slip event (SSE) over the past ten years, but without information on duration, we cannot single out LFEs in our region. SSEs are the physical process in a subduction zone that produces “slow earthquakes” which characterize LFEs and VLEs. 

All variables can be used to gauge changes in Cascadia’s seismic behavior. An approach to “The Big One” may be indicated with increases in magnitude and occurrence over time, and the types of earthquakes (shallow or slow). Investigating data about the past may point out patterns about future events in this region. 


## Methods: 

Seismic data for this paper was taken from the U.S Geological Survey Earthquake Catalog (USGS). The USGS data was selected because USGS research is used by all levels of government and the private sector; any scholarly publications that go through USGS are required to be public access and are limited to research conducted by USGS or funded by USGS. This ensures precision and validity of data and easy accessibility.

RStudio was utilized in data storage, filtration, and in the creation of data visualizations. RStudio is free, open-source software, useful for exploratory data analysis (EDA). The object-oriented programming language R is efficient to organize and manipulate multiple datasets in a single environment. I considered SAS when determining what software to run my EDA in, depending on the size of my datasets; SAS is efficient for big data while R is not. R can run many packages that allow the user to apply functions on data dependent on the package. The packages “ggplot2”, “gridExtra”, “tidyr”, “dplyr”, "shiny" and "leaflet" were used for this report.

ArcGIS Online was used to create additional web-maps. ArcGIS is a cloud-based software created by Esri that builds interactive, accessible, online web maps. It offers intuitive analysis tools and specializes in location data and can host multiple types of datasets, from GeoJSON files to .CSV. It is not an open-source software and does not offer a variety of options for visualization output; this is limited to a PDF or a web map. 

Datasets were obtained from the USGS Earthquake catalog. This allows the user to filter data by factors (i.=e., location, interval of time, minimum or maximum magnitude of earthquake, type of seismic event), and offers KML, .CSV, GeoJSON, QuakeML, and Map & List formats. 
.CSV files were chosen because of the use of RStudio that can easily read in .csv files with the “read.csv” function. CSZ data was taken with coordinates between 38.000 to 47.000° N and -122.000 to -125.000° W, with magnitudes greater than 2.5. NSAF data was taken with coordinates 36.683 to 40.411° N and -124.456 to -119.993° W Since VLFs have a magnitude around 3, this does accommodate any slow earthquakes. Our data doesn’t contain information about the duration of earthquakes relative to magnitude, so we are assuming all earthquakes in our data are shallow earthquakes. 
Each data set contains data corresponding to 22 different variables and filtered to 14 variables. Variables kept include “time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst". Not all kept variables were used in EDA. Many variables removed contained data not pertaining to the purpose of the data, but the validity of it, i.e, “magError” and “status”. MagError is the standard error for the magnitude, and status indicates whether a datapoint has been reviewed by a human. The core variables of this set include “depth”, “mag”, “latitude”, “longitude”, and “time”. Additional variables kept were for the purpose of future analyses. However, variables like “nst”, “gap”, “dmin”, “rms”, and “net” also hold information regarding distance from the event to the nearest seismic station, the root mean square error, and more on the validity of the data point. 

## Results

### Power Law Distribution
Earthquake data generally follows a **power law distribution**.

![Image 4: Power Law Distribution. Graphic by VISIBLE](PowerLaw.png){width=50%}


Power law distributions show relative change in one quantity resulting in a proportional relative power change in another quantity. If a square’s length and area followed a power law relationship, then doubling the length would increase the area by a factor of four. 
Earthquake magnitude and frequency follow a power law distribution, where there is a proportional relationship between magnitude and frequency. As the magnitude of earthquakes in a defined area increases, the frequency of occurrence decreases. In seismology, the Gutenberg-Richter law expresses this relationship as:
$$
N = 10^{(a-bM)}
$$
where M is magnitude. N is the number of events with a magnitude greater than or equal to M, and a and b are constants. 


### Magnitude


Figure 1 below displays the distribution of magnitude for each year for CSZ and Figure 2 for NSAF:

```{r echo = FALSE, message = FALSE, include = FALSE}
## reading in all our data 
getwd()
setwd("C:/Users/sweth/Desktop/HON498 Materials")
library(grid)
library(gridExtra)

eq2021 <- read.csv(file = "C:/Users/sweth/Desktop/HON498 Materials/cszeq2021.csv")
eq2020 <- read.csv(file = "C:/Users/sweth/Desktop/HON498 Materials/query (1).csv")
eq2019 <- read.csv(file = "C:/Users/sweth/Desktop/HON498 Materials/query (3).csv")
eq2018 <- read.csv(file = "C:/Users/sweth/Desktop/HON498 Materials/query (4).csv")
eq2017 <- read.csv(file = "C:/Users/sweth/Desktop/HON498 Materials/query (5).csv")
eq2016 <- read.csv(file = "C:/Users/sweth/Desktop/HON498 Materials/cszeq2016.csv")
eq2015 <- read.csv(file = "C:/Users/sweth/Desktop/HON498 Materials/cszeq2015.csv")
eq2014 <- read.csv(file = "C:/Users/sweth/Desktop/HON498 Materials/cszeq2014.csv")
eq2013 <- read.csv(file = "C:/Users/sweth/Desktop/HON498 Materials/cszeq2013.csv")
eq2012 <- read.csv(file = "C:/Users/sweth/Desktop/HON498 Materials/cszeq2012.csv")
eq2011 <- read.csv(file = "C:/Users/sweth/Desktop/HON498 Materials/cszeq2011.csv")


# subsetting the data; keeping only the variables needed

eq2021<- subset(eq2021, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
eq2020 <- subset(eq2020, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
eq2019 <- subset(eq2019, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
eq2018 <- subset(eq2018, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
eq2017 <- subset(eq2017, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
eq2016 <- subset(eq2016, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
eq2015 <- subset(eq2015, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
eq2014 <- subset(eq2014, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
eq2013 <- subset(eq2013, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
eq2012 <- subset(eq2012, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
eq2011 <- subset(eq2011, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
# converting values of time for time series
eq2020$time <-as.POSIXct(eq2020$time,"%Y-%m-%d %H:%M:%S")
eq2021$time <-as.POSIXct(eq2021$time,"%Y-%m-%d %H:%M:%S")
eq2019$time <-as.POSIXct(eq2019$time,"%Y-%m-%d %H:%M:%S")
eq2018$time <-as.POSIXct(eq2018$time,"%Y-%m-%d %H:%M:%S")
eq2017$time <-as.POSIXct(eq2017$time,"%Y-%m-%d %H:%M:%S")
eq2016$time <-as.POSIXct(eq2016$time,"%Y-%m-%d %H:%M:%S")
eq2015$time <-as.POSIXct(eq2015$time,"%Y-%m-%d %H:%M:%S")
eq2014$time <-as.POSIXct(eq2014$time,"%Y-%m-%d %H:%M:%S")
eq2013$time <-as.POSIXct(eq2013$time,"%Y-%m-%d %H:%M:%S")
eq2012$time <-as.POSIXct(eq2012$time,"%Y-%m-%d %H:%M:%S")
eq2011$time <-as.POSIXct(eq2011$time,"%Y-%m-%d %H:%M:%S")

# Reading the NSAF Data in
eqsa2021 <- read.csv(file = "C:/Users/sweth/Desktop/HON499 Materials/NSAF2021.CSV")
eqsa2020 <- read.csv(file = "C:/Users/sweth/Desktop/HON499 Materials/NSAF2020.CSV")
eqsa2019 <- read.csv(file = "C:/Users/sweth/Desktop/HON499 Materials/NSAF2019.CSV")
eqsa2018 <- read.csv(file = "C:/Users/sweth/Desktop/HON499 Materials/NSAF2018.CSV")
eqsa2017 <- read.csv(file = "C:/Users/sweth/Desktop/HON499 Materials/NSAF2017.CSV")
eqsa2016 <- read.csv(file = "C:/Users/sweth/Desktop/HON499 Materials/NSAF2016.CSV")
eqsa2015 <- read.csv(file = "C:/Users/sweth/Desktop/HON499 Materials/NSAF2015.CSV")
eqsa2014 <- read.csv(file = "C:/Users/sweth/Desktop/HON499 Materials/NSAF2014.CSV")
eqsa2013 <- read.csv(file = "C:/Users/sweth/Desktop/HON499 Materials/NSAF2013.CSV")
eqsa2012 <- read.csv(file = "C:/Users/sweth/Desktop/HON499 Materials/NSAF2012.CSV")
eqsa2011 <- read.csv(file = "C:/Users/sweth/Desktop/HON499 Materials/NSAF2011.CSV")

# subsetting the data; keeping only the variables needed
eqsa2021 <- subset(eqsa2021, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
eqsa2020 <- subset(eqsa2020, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
eqsa2019 <- subset(eqsa2019, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
eqsa2018 <- subset(eqsa2018, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
eqsa2017 <- subset(eqsa2017, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
eqsa2016 <- subset(eqsa2016, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
eqsa2015 <- subset(eqsa2015, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
eqsa2014 <- subset(eqsa2014, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
eqsa2013 <- subset(eqsa2013, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
eqsa2012 <- subset(eqsa2012, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
eqsa2011 <- subset(eqsa2011, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst"))
eqsa2021$time <-as.POSIXct(eqsa2021$time,"%Y-%m-%d %H:%M:%S")
eqsa2020$time <-as.POSIXct(eqsa2020$time,"%Y-%m-%d %H:%M:%S")
eqsa2019$time <-as.POSIXct(eqsa2019$time,"%Y-%m-%d %H:%M:%S")
eqsa2018$time <-as.POSIXct(eqsa2018$time,"%Y-%m-%d %H:%M:%S")
eqsa2017$time <-as.POSIXct(eqsa2017$time,"%Y-%m-%d %H:%M:%S")
eqsa2016$time <-as.POSIXct(eqsa2016$time,"%Y-%m-%d %H:%M:%S")
eqsa2015$time <-as.POSIXct(eqsa2015$time,"%Y-%m-%d %H:%M:%S")
eqsa2014$time <-as.POSIXct(eqsa2014$time,"%Y-%m-%d %H:%M:%S")
eqsa2013$time <-as.POSIXct(eqsa2013$time,"%Y-%m-%d %H:%M:%S")
eqsa2012$time <-as.POSIXct(eqsa2012$time,"%Y-%m-%d %H:%M:%S")
eqsa2011$time <-as.POSIXct(eqsa2011$time,"%Y-%m-%d %H:%M:%S")

```


```{r echo = FALSE, message = FALSE}
library(ggplot2)
library(grid)
## CSZ 
## we are going to create all our histograms and mend with grid.arrange()
hist1 <- ggplot(eq2011, aes(x = mag)) + geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666") + ggtitle("2011") +theme(text = element_text(size = 5), axis.title = element_text(size =6))
hist2 <- ggplot(eq2012, aes(x = mag)) + geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666") + ggtitle("2012") +theme(text = element_text(size = 5), axis.title = element_text(size =6))
hist3 <- ggplot(eq2013, aes(x = mag)) + geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666") + ggtitle("2013") +theme(text = element_text(size = 5), axis.title = element_text(size =6))
hist4 <- ggplot(eq2014, aes(x = mag)) + geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666") + ggtitle("2014") +theme(text = element_text(size = 5), axis.title = element_text(size =6))
hist5 <- ggplot(eq2015, aes(x = mag)) + geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666") + ggtitle("2015") +theme(text = element_text(size = 5), axis.title = element_text(size =6))
hist6 <- ggplot(eq2016, aes(x = mag)) + geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666") + ggtitle("2016") +theme(text = element_text(size = 5), axis.title = element_text(size =6))
hist7 <- ggplot(eq2017, aes(x = mag)) + geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666") + ggtitle("2017") +theme(text = element_text(size = 5), axis.title = element_text(size =6))
hist8 <- ggplot(eq2018, aes(x = mag)) + geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666") + ggtitle("2018") +theme(text = element_text(size = 5), axis.title = element_text(size =6))
hist9 <- ggplot(eq2019, aes(x = mag)) + geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666") + ggtitle("2019") +theme(text = element_text(size = 5), axis.title = element_text(size =6))
hist10 <- ggplot(eq2020, aes(x = mag)) + geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666") + ggtitle("2020") +theme(text = element_text(size = 5), axis.title = element_text(size =6))
hist11 <- ggplot(eq2021, aes(x = mag)) + geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666") + ggtitle("2021") +theme(text = element_text(size = 5), axis.title = element_text(size =6))

grid.arrange(hist1, hist2, hist3, hist4, hist5, hist6, hist7, hist8, hist9, hist10, hist11, top=textGrob("Figure 1: Distribution of CSZ Earthquakes from 2011 to 2021", gp=gpar(fontsize=7)))

##NSAF 
sa_hist1 <- ggplot(eqsa2011, aes(x = mag)) + geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666") + ggtitle("2011") +theme(text = element_text(size = 5), axis.title = element_text(size =6))
sa_hist2 <- ggplot(eqsa2012, aes(x = mag)) + geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666") + ggtitle("2012") +theme(text = element_text(size = 5), axis.title = element_text(size =6))
sa_hist3 <- ggplot(eqsa2013, aes(x = mag)) + geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666") +ggtitle("2013") +theme(text = element_text(size = 5), axis.title = element_text(size =6))
sa_hist4 <- ggplot(eqsa2014, aes(x = mag)) + geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666")+ ggtitle("2014") +theme(text = element_text(size = 5), axis.title = element_text(size =6))
sa_hist5 <- ggplot(eqsa2015, aes(x = mag)) + geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666") +ggtitle("2015") +theme(text = element_text(size = 5), axis.title = element_text(size =6))
sa_hist6 <- ggplot(eqsa2016, aes(x = mag)) + geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666") +ggtitle("2016") +theme(text = element_text(size = 5), axis.title = element_text(size =6))
sa_hist7 <- ggplot(eqsa2017, aes(x = mag)) + geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666")+ggtitle("2017") +theme(text = element_text(size = 5), axis.title = element_text(size =6))
sa_hist8 <- ggplot(eqsa2018, aes(x = mag)) + geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666") +ggtitle("2018") +theme(text = element_text(size = 5), axis.title = element_text(size =6))
sa_hist9 <- ggplot(eqsa2019, aes(x = mag)) + geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666") + ggtitle("2019") +theme(text = element_text(size = 5), axis.title = element_text(size =6))
sa_hist10 <- ggplot(eqsa2020, aes(x = mag)) +geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666") + ggtitle("2020") +theme(text = element_text(size = 5), axis.title = element_text(size =6))
sa_hist11 <- ggplot(eqsa2021, aes(x = mag)) +geom_histogram(aes(y=..density..), colour = "black", fill = "white", bins = 20) + geom_density(alpha = .2, fill = "#FF6666") + ggtitle("2021") +theme(text = element_text(size = 5), axis.title = element_text(size =6))

grid.arrange(sa_hist1, sa_hist2, sa_hist3, sa_hist4, sa_hist5, sa_hist6, sa_hist7, sa_hist8, sa_hist9, sa_hist10, sa_hist11, top=textGrob("Figure 2: Magnitude vs. Time for NSAF Earthquakes from 2011 to 2021", gp=gpar(fontsize=7)))
```

Both figures show consistent results that the magnitude of each year for both locations follow power law distributions. Hence, tests with normality assumptions will not be applicable here. It should be noted that data for 2020 in CSZ nearly follows a uniform distribution, which could be do to a drastic decrease in seismic events that year due to the universal 'Quiet Period', discussed later in this section. 

### Time

Our previous figures only show the probability of each magnitude. Figure 3 shows the changes in magnitude over time. 

```{r echo = FALSE, message= FALSE, warning= FALSE}
#CSZ
layout_matrix_1 <- matrix(1:6, ncol = 3)
layout(layout_matrix_1)
plot.new()
title("Figure 4: Earthquakes in \n CSZ from 2011-2021", line = -2)

plot(x = eq2013$time, y = eq2013$mag, type = "h", col = "red", main = "2013", xlab = "Time", ylab ="Mag")
plot(x = eq2011$time, y = eq2011$mag, type = "h", col = "red", main = "2011", xlab = "Time", ylab ="Mag")
plot(x = eq2014$time, y = eq2014$mag, type = "h", col = "red", main = "2014", xlab = "Time", ylab ="Mag")
plot(x = eq2012$time, y = eq2012$mag, type = "h", col = "red", main = "2012", xlab = "Time", ylab ="Mag")
plot(x = eq2015$time, y = eq2015$mag, type = "h", col = "red", main = "2015", xlab = "Time", ylab ="Mag")
plot(x = eq2016$time, y = eq2016$mag, type = "h", col = "red", main = "2016", xlab = "Time", ylab ="Mag")
plot(x = eq2019$time, y = eq2019$mag, type = "h", col = "red", main = "2019", xlab = "Time", ylab ="Mag")
plot(x = eq2017$time, y = eq2017$mag, type = "h", col = "red", main = "2017", xlab = "Time", ylab ="Mag")
plot(x = eq2020$time, y = eq2020$mag, type = "h", col = "red", main = "2020", xlab = "Time", ylab ="Mag")
plot(x = eq2018$time, y = eq2018$mag, type = "h", col = "red", main = "2018", xlab = "Time", ylab ="Mag")
plot(x = eq2021$time, y = eq2021$mag, type = "h", col = "red", main = "2021", xlab = "Time", ylab ="Mag")
```


```{r echo = FALSE, message = FALSE, warning = FALSE}
##NSAF
layout(layout_matrix_1)
plot.new()
title("Figure 5: Earthquakes in \n NSAF from 2011-2021", line = -2)
plot(x = eqsa2013$time, y = eqsa2013$mag, type = "h", col = "royalblue1", main = "2013", xlab = "Time", ylab ="Mag")
plot(x = eqsa2011$time, y = eqsa2011$mag, type = "h", col = "royalblue1", main = "2011", xlab = "Time", ylab ="Mag")
plot(x = eqsa2014$time, y = eqsa2014$mag, type = "h", col = "royalblue1", main = "2014", xlab = "Time", ylab ="Mag")
plot(x = eqsa2012$time, y = eqsa2012$mag, type = "h", col = "royalblue1", main = "2012", xlab = "Time", ylab ="Mag")
plot(x = eqsa2015$time, y = eqsa2015$mag, type = "h", col = "royalblue1", main = "2015", xlab = "Time", ylab ="Mag")
plot(x = eqsa2016$time, y = eqsa2016$mag, type = "h", col = "royalblue1", main = "2016", xlab = "Time", ylab ="Mag")
plot(x = eqsa2019$time, y = eqsa2019$mag, type = "h", col = "royalblue1", main = "2019", xlab = "Time", ylab ="Mag")
plot(x = eqsa2017$time, y = eqsa2017$mag, type = "h", col = "royalblue1", main = "2017", xlab = "Time", ylab ="Mag")
plot(x = eqsa2020$time, y = eqsa2020$mag, type = "h", col = "royalblue1", main = "2020", xlab = "Time", ylab ="Mag")
plot(x = eqsa2018$time, y = eqsa2018$mag, type = "h", col = "royalblue1", main = "2018", xlab = "Time", ylab ="Mag")
plot(x = eqsa2021$time, y = eqsa2021$mag, type = "h", col = "royalblue1", main = "2021", xlab = "Time", ylab ="Mag")

```

These plots were created to show gaps of time between earthquakes, in comparison to a time-series chart that can illustrate false continuity of events. They also show clustering to indicate build-up of stress in the fault [see March 2014 in CSZ data]. 

Something to note in the data is the June 2019 5.5 earthquake and the 6.0 earthquake in December 2021 were picked up by seismic stations in both regions due to the location of the event in the Mendocino Triple Junction and the location of seismic stations.

The general pattern in these visualizations is the appearance of clusters around a higher magnitude earthquake. During an earthquake, plates moving across the surface cause stress to build up and release seismic waves from pent up energy; that build-up of stress can be seen through these small clusters and are visible in nearly every year. 
```{r echo = FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(GGally)
masterCSZ <- read.csv(file = "masterCSZ.csv")
masterNSAF <- read.csv(file = "masterNSAF.csv")
masterNSAF <- subset(masterNSAF, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst")) 
masterCSZ <- subset(masterCSZ, select = c("time", "latitude", "longitude", "depth", "mag", "magType", "nst", "gap", "dmin", "rms", "net", "place", "type",  "magNst")) 
ggpairs(masterCSZ[4:5], 
        title = "Scatterplot Matrix 1: CSZ Events" , size = 1)
ggpairs(masterNSAF[4:5], 
        title = "Scatterplot Matrix 2: NSAF Events", size = 1)
```

Scatterplot matrices visualize the relationship between depth and magnitude. The correlation coefficient noted on the graph indicates the strength of the relationship between these two variables. As aforementioned, magnitude and depth do have a relationship related to the intensity of an earthquake, so we expect to see a relationship between our variables. 

Scatterplot Matrix 2 of NSAF events shows a correlation of 0.016, meaning that there is nearly a 1 in 100 chance we would see these observations if our variables were not related. The correlation coefficient is even lower for CSZ events in Matrix 1 at 0.002. Both plots show a strong relationship between lower values in the depth (closer to the surface) and lower magnitude, a relationship also seen by our power law distributions. 

Conclusions about the data are difficult to make from analyzing these plots side by side. Statistical testing can be used to compare magnitude between the groups.

### Statistical Tests

To compare magnitude between the two groups over a 10-year period, we can perform a **Wilcoxon Rank Sum test**, also known as the **Mann Whitney U test**. This is a non-parametric test used when data are robust and assumes the data are distribution free, based on rank values, and independence between groups. **Ranking** is the process of transforming qualitative data by replacing them with their assigned placement in relation to another. For example, if the data are 3, 6, 2, their ranks would be 2, 3, and 1 respectively.

When comparing the means of two groups, the t-test is often used with assumptions of normality, random sampling, and large sample size to compare population means. Since our magnitude does not follow a normal distribution, using a t-test would be extrapolation of our data and lead to insignificant conclusions. 

The formula for the Mann Whitney U test is as seen below: 

$$\ 
 U = n_1n_2 + \frac{(n_1)(n_1 + 1)}{2} - R_1
 $$
Where $n_1n_2$ are the sample space, $\frac{(n_1)(n_1 + 1)}{2}$ are the theoretical sum of all ranks for group A and $R_1$ are the actual sum of all ranks for group A. 

$\ H_0:$ There is no significant difference between the median magnitude of NSAF and CSZ over a ten year period

$\ H_a:$ There is a significant difference between the median magnitude of NSAF and CSZ over a ten year period 

This test will be evaluated with **alpha** ($\alpha$), which is the probability that the event in question occurred by chance. The **p-value** is a value from a statistical test that describes the probability of finding the observed results when the null hypothesis is true. 

If the probability of an event occurring by chance is higher than the probability of it occurring from our observed results, we know we can reject the null hypothesis. We'll be using alpha = .05 for this test, and will reject if alpha is higher than our p-value.

```{r echo= FALSE, message = FALSE, warning = FALSE, include = FALSE}
# creating a data frame of the csz and nsaf magnitudes. 

wilcox_csz_set <-data.frame(masterCSZ[ , c("mag")])
wilcox_nsaf_set <- data.frame(masterNSAF[ , c("mag")])
wilcox_set <- merge(wilcox_csz_set, wilcox_nsaf_set) # this takes a bit to run

colnames(wilcox_set)[1] = "CSZ"
colnames(wilcox_set)[2] = "NSAF"
```


```{r}
# performing our test
wilcox.test(wilcox_set$CSZ, wilcox_set$NSAF, alternative = "two.sided")
```
The test results in a p-value less than 2.2*10-16. Since our p-value is less than $\alpha = .05$, we reject the null hypothesis and find evidence in support of the alternative. 

To compare and predict the number of future earthquake events, we can assume a **Poisson Distribution** and use its **probability mass function (pmf)**. This is function used to determine the probability that a discrete random variable equals some specified value; it can make predictions on the likelihood of an event of a specific magnitude occurring. Earthquakes do occur randomly but over a long period of times they approach a constant rate. The distribution assumes countable, independent events that do not occur simultaneously, and that the event maintains a homogeneous rate of occurrence (an average rate of which events occur can be found). 

The Poisson distribution is generally used in earthquake research but presents many issues. Assuming the average rate of earthquakes is consistent over time does not account for external factors that can trigger earthquakes (i.e., eruptions, fracking). 

The 'Quiet Period' of 2020 represents this flaw; research has shown COVID-19 lockdown measures, which lead to decreased economic, industrial, and travel activity, accounted for a 50% reduction in seismic noise around the world. Our CSZ data shows a record low 44 earthquakes picked up during this time (McGill University, 2020).

We chose to run the Poisson distribution to make predictions on 1, 5, and 10 year intervals keeping this in mind with both the CSZ, NSAF, and combined data sets. 
 
To begin this calculation we first must find $\λ$ , the rate of the event that a magnitude 5.5 or greater earthquake has occurred in the CSZ with our data from the last 10 years. An earthquake of this magnitude causes visible, slight damage to physical structures, which is why it was chosen as a baseline.



```{r include = FALSE, echo = FALSE, message = FALSE}
# what is the probability that the next earthquake to hit the csz with seismic intensity 6.0 will hit within one year?
# find the number of events greater than 5 that occurred in the last 10 years
sum(masterCSZ$mag >='5.5') 
6/10
```

There are six events over our 10 year period with magnitudes greater than or equal to 5.5. Divided by 10, this gives us $\ λ = .60 $. 

To determine the probability that the next earthquake with magnitude greater than 5.5, we first find the probability that it doesn't occur in one year, and subtract that from 1. 

R has multiple built-in Poisson functions. Each function applies different depending on what the user requires, but for the sake of simplicity, the Poisson PMF has been hard coded for this analysis as pois(). Calculations were double checked with the Python package 'numpy' and images of the code are included. Both R and Python produced equivalent results.

```{r}
pois = function(lambda, span, x) {
    a <- (exp(-span*lambda)*(span*lambda)^x)/factorial(x)
    b = 1-a
    print(b)
}
pois(6/10, 1, 0)
pois(6/10, 5, 0)
pois(6/10,10,0)
```

This results in 0.45119, or an approximately **45.1% chance of a magnitude 5.5 or greater earthquake occurring in the Cascadia area in the next year.**

The same function can be used to calculate the probability for 5 and 10 years by adjusting the value of 'span' accordingly. An input of 5 gives us **95.02%.** An input of 10 gives us **99.75%.**

![Image 5: Poisson PMF in Python](pythoncode.png){width=50%}

![Image 6: Poisson PMF in Python](pythoncode2.png){width=200px}
![Image 7: Poisson PMF in Python](pythoncode3.png){width=200px}
Running the same procedure for NSAF gives the following results:

```{r, echo = FALSE, include = FALSE}
sum(masterNSAF$mag >='5.5')
pois(4/10,1,0)
pois(4/10,5,0)
pois(4/10,10,0)
```

```{r echo = FALSE, results = 'asis'}
library(ggplot2)

location<- c("CSZ","CSZ","CSZ", "NSAF","NSAF","NSAF")

year <- 
  c(1, 5, 10, 1, 5, 10)

probs <- 
  c(45.10, 95.02,99.75, 32.96, 86.46, 98.16)

pois_tbl <- data.frame(location, year, probs)

#table
library(knitr)
kable(pois_tbl, caption = "Table of Poisson Probabilities by Year")
#barchart
ggplot(pois_tbl, aes(fill = location, y = probs, x = year))+geom_bar(position = "dodge", stat = "identity") + ggtitle("Poisson Probabilities") +scale_fill_manual(values = c("#264653", "#2a9d8f"))
```

When performing hypothesis testing, we must consider room for **Type I** and **Type II** errors, even in nonparametric tests. A Type I error happens when a test rejects the null hypothesis when the null is true, and Type II when a test fails to reject the null when it is not true. **Power** is the probability of correctly rejecting the null. We have to consider the possibility of Type II error if a nonparametric test fails to reject $\ H_0:$ because nonparametric tests are subject to low power from their low sample size. There is not much literature on calculating Type I and Type II errors in nonparametric testing. 


## Negative Binomial

The Negative Binomial distribution can also be used in earthquake forecasting. This distribution models the number of failures in a series of independent and identically distributed Bernoulli trials. **Bernoulli Trials** are trials defined by their two outcomes: success and failure, and independent and finite trials. An example is if landing a "heads" on a coin flip is considered a success, asking how many times will we flip a coin before we earn three "heads" (three successes). Because of the limitations of the Poisson distribution in earthquake forecasting, literature suggests using the negative binomial as an alternative. Considering the occurrence of a magnitude 5.5 earthquake occurring in either region as a success, we can find the probability of the amount of times an earthquake occurs before we get a certain number of success. We will test the probability that a magnitude 5.5 earthquake occurs 3 times out of 5, 10, and 15 earthquake occurrences. 
```{r}
#r = number of successes
#p = .60 
#n = x(number of occurrences)-r # number of successes
#csz
dnbinom(x = 2, size = 3 , prob = 0.6) # 5 eqs
dnbinom(x = 7, size = 3, prob = 0.6) # 10 eqs
dnbinom(x = 12, size = 3, prob = 0.6) #15 eqs
```


```{r, include = FALSE}
#NSAF
dnbinom(x = 2, size = 3 , prob = 0.4) # 5 eqs
dnbinom(x = 7, size = 3, prob = 0.4) # 10 eqs
dnbinom(x = 12, size = 3, prob = 0.4) #15 eqs
```

This tells us that the probability of having 3 'successes' of a magnitude 5.5 earthquake or higher occurring in CSZ after 2 failures is **20%**. The probability of 3 successes after 7 failures is **1.27%**. After 12 failures is it **0.03297%**. A pattern of lower probabilities are present the higher the number of failures. We can determine if there is any change in this pattern if we change the number of successes. We can calculate probabilities for NSAF in the same way, with 'prob = 0.4'

Letting r, the number of successes equal 5, we get the following: 
```{r}
#CSZ
dnbinom(x = 0, size = 5 , prob = 0.6) # 5 eqs
dnbinom(x = 5, size = 5, prob = 0.6) # 10 eqs
dnbinom(x = 10, size = 5, prob = 0.6) #15 eqs
```


```{r, include = FALSE}
#NSAF
dnbinom(x = 0, size = 5 , prob = 0.4) # 5 eqs
dnbinom(x = 5, size = 5, prob = 0.4) # 10 eqs
dnbinom(x = 10, size = 5, prob = 0.4) #15 eqs
```

This pattern of lower probabilities the higher the amount of failures is not present when we change 'r' for CSZ or NSAF. When we calculate the probability that the number of successes, 5, occurs in 5 trials, we get a probability of **7.77%**.

The table and bar chart below illustrates our probabilities for each region:

```{r, echo = FALSE, message = FALSE}
#location<- c("CSZ","CSZ","CSZ", "NSAF","NSAF","NSAF")

occurrences <- 
  c(5, 10, 15, 5, 10, 15)

nb_probs <-
  c(20.73, 1.27,0.0329,13.84,6.44,1.26)

nb_tbl <- data.frame(location, occurrences, nb_probs)

kable(nb_tbl, caption = "Table of Negative Binomial Probabilities by Occurrences (r = 3)")
ggplot(nb_tbl, aes(fill = location, y = nb_probs, x = occurrences))+ geom_bar(position = "dodge", stat = "identity") + ggtitle("Negative Binomial Probabilities", subtitle = "r = 3") + scale_fill_manual(values = c("#e26d5c", "#723d46")) + ylab("probs")

```

The following table and graph represents our results when r = 5 in the number of successes: 

```{r, echo = FALSE, message = FALSE}
#location<- c("CSZ","CSZ","CSZ", "NSAF","NSAF","NSAF")

#occurrences <- 
  #c(5, 10, 15, 5, 10, 15)

nb_probs2 <-
  c(7.77,10.03,0.8161,1.024,10.032,6.197)

nb_tbl2 <- data.frame(location, occurrences, nb_probs2)

kable(nb_tbl2, caption = "Table of Negative Binomial Probabilities by Occurrences (r = 5)")
ggplot(nb_tbl2, aes(fill = location, y = nb_probs2, x = occurrences))+ geom_bar(position = "dodge", stat = "identity") + ggtitle("Negative Binomial Probabilities", subtitle = "r = 5") + scale_fill_manual(values = c("#db7c26", "#c32f27")) + ylab("probs")

```



## K Nearest-Neighbors

**K Nearest-Neighbors** (k-NN) is a nonparametric supervised machine learning algorithm that is used for data classification to estimate the chances a data point will become a member of one of two or more groups. It can highlight patterns of classification through the predictions in makes without making any assumptions about the underlying distribution of the data. The classification process is only possible with discrete values, which are used to calculate the Euclidean distance between data points. The formula for this is as seen below:

$\sqrt{(X_1-X_2)^2 + (Y_1-Y_2)^2}$

Say we want to predict in what region, NSAF or CSZ, a magnitude 5.5 earthquake with a depth of 12 would appear. k-NN will find the distance from each of our data points to this value to determine the likelihood of this value appearing in either group. The points closest to the value we want to predict is known as the *nearest neighbor*

We will be using magnitude and depth as our predictor variables and our groups as NSAF and CSZ. Categorical variables CSZ and NSAF will be coded as 1 and zero respectively to build the model. Our first step will be combining our 'master' data sets; these contain all data and attributes from 2011-2021 for CSZ and NSAF. 



```{r}
library(class)
# combining our data sets

# here we create a column that gives all values in the CSZ data set "1" to indicate location numerically 
masterCSZ$group<-replicate(2435,1)
masterNSAF$group<-replicate(2644, 0) # and repeat for NSAF
masterset<- rbind(masterCSZ, masterNSAF) # choosing to use rbind here instead of merge to ensure there's no overlap of values
masterset <- masterset[sample(1:nrow(masterset)),] # randomly reordering our rows to create our training and test data sets
masterset <- masterset[1:100, ] # using the entirety of the rows will break my R (I know this because it did), so cropping the data set at 100 values. 

# subset for simplicity
masterset<-subset(masterset, select = c("depth", "mag", "group")) #removing unnecessary variables
```


```{r include =FALSE}
na.omit(masterset) # making sure there are no missing values 
```

To make predictions on the classification of new data from our original data, kNN uses a constant 'k' to indicate the number of nearest points or "nearest neighbors" to our predicted data. This is done by ranking the Euclidean distance between points; k is an indication of how many points will be ranked to determine the classification of new data. The most optimal way to calculate k is to take the square root of the number of observations in our data set, which is 70 in our case. For the sake of simplicity, we will be using k = 10. 
```{r include = FALSE}
NROW(masterset)
sqrt(5079)
# so we can try using around k = 70
```

In using this algorithm, it is recommended to normalize the data prior to analysis to account for issues in the way our data is associated with each group. However, the grouping of our data is pre-determined in the process of collecting the data [done by USGS and their seismic stations], so it is not necessary for us to normalize the data because our parameters of magnitude and depth do not determine what group our data are classified as in data collection.

To test the accuracy of our model, we first test it on the data we have. We will do this by splitting 'masterset' into two data sets: our training and testing data sets. We will split our set as such:

```{r}
master_train<- masterset[1:45,]
master_test<-masterset[46:100,]
master_train_labels <-masterset[1:45,3]
master_test_labels<- masterset[46:100,3]
```

R has a built in "knn()" function to classify the data:

```{r}
masterset_pred <-knn(train = master_train, test = master_test, cl = master_train_labels, k = 10)
```


The model is built but to test the accuracy of the predicted values and whether they match up with the values in 'master_test_labels' by creating a cross table: 

```{r}
library(gmodels)
CrossTable(x = master_test_labels, y = masterset_pred, prop.chisq = FALSE)
```
Our test data consisted of 55 observations, and out of those 55, 32 values have been accurately predicted as being located in NSAF. 10 out of 55 observations were predicted correctly as being located in CSZ. Out of the 55 values, 40 came from NSAF and 15 from CSZ. There were 5 values that were actually from NSAF that were marked as from CSZ. The total accuracy of this model is the number of correct classifications divided by the total observations. This is (32 + 10)/55, which is about **76.3%**. There's room to improve model performance if we change the value of k, however, this is fairly fitting model. This model can then be applied on real test data to classify certain earthquakes by magnitude and depth with CSZ and NSAF. 

Applying our model onto a data point with a depth of 12 and a magnitude of 5.5:
```{r}
mytest <- master_train[1,-5]
mytest[1,] <- c(12, 5.5, 3)
knn(train = master_train, test = mytest, cl = master_train_labels, k = 10)
```

An earthquake with a magnitude 5.5 and a depth of 12km is predicted to occur in NSAF. 

The table below shows a few examples of this and the predicted area of earthquake rupture by the model:

```{r, echo =  FALSE, message = FALSE, warning = FALSE}
library(ggplot2)
mytest <- master_train[1,-5]
#csz = 1, nsaf = 0
mytest[1,] <- c(17, 9.0, 3)
knn(train = master_train, test = mytest, cl = master_train_labels, k = 10)
```


```{r, echo =  FALSE, message = FALSE, warning = FALSE}
location<- c("CSZ","CSZ","NSAF", "NSAF","CSZ","CSZ", "CSZ")

depth <- 
  c(28, 60, 10, 13, 21, 33, 17)

magnitude <- 
  c(3.0, 6.2, 4.3, 5.0, 2.4, 7.0, 9.0)

predict_tbl <- data.frame(depth, magnitude, location)

#table
library(knitr)
kable(predict_tbl, caption = "Earthquake Location Predictions from kNN Model")

ggplot(data = predict_tbl, aes(x = depth, y= magnitude, fill = location)) +geom_bar(stat = "identity") +scale_fill_manual(values = c("#054C70","#05C3DE")) + ggtitle("kNN Predictions")

```

An interesting observation is our model classifies an earthquake of depth 33 km and magnitude 7.0 as occurring in CSZ, as well as a magnitude 9.0 with depth 17 km. Shallow earthquakes range from 0-70 km, intermediate earthquakes from 70-300 km, and deep earthquakes from 300-700 km. As aforementioned, our model only holds records for shallow earthquakes due to the size and placement of seismic stations. The model takes into account the number of observations, depth, and magnitude of both CSZ and NSAF earthquakes; for the model to predict that it's more likely an earthquake of this severity to occur in CSZ means there's patterns in our depth and magnitude that point to lower depth and higher magnitude quakes. This attests to predictions that CSZ is susceptible to a "megathrust" earthquake in the next 50 years (The Earthquake That Will Devastate the Pacific Northwest). Considering this prediction is made with data that covers a 10 year time frame, it implies high chances of this earthquake happening. The model limits us through its 76.3% accuracy, which was considered when analyzing output.

## Spatial Visualizations

To get a better understanding of the data, we can use packages in R to view earthquake events on a map. There are a few packages to do this in R, but "mapview" proved the most successful with this data. Creating an RShiny is also possible, but complications in the string value of the 'time' variable inhibited an RShiny from having dynamic visualizations. The user can click on the "layer" icon (appears as stacked planes on the left side of the visual) to filter the map. 

```{r, echo = FALSE, message= FALSE, warning=FALSE}

library(mapview)
library(tidyverse)
masterCSZ$yr <- format(as.POSIXct(masterCSZ$time,"%Y-%m-%d %H:%M:%S"), "%Y")

framemasterCSZ <- mapview(masterCSZ, alpha = 0, xcol = "longitude", ycol = "latitude", zcol = c("mag", "yr"),crs = 4269, grid = FALSE, map.type = "CartoDB.Positron", layer.name = "Magnitude", legend.opacity = .5, col.regions = hcl.colors(14, palette = list( "Lajolla", "blue")))
 
framemasterCSZ

masterNSAF$yr <- format(as.POSIXct(masterNSAF$time,"%Y-%m-%d %H:%M:%S"), "%Y")

framemasterNSAF <- mapview(masterNSAF, alpha = 0, xcol = "longitude", ycol = "latitude", zcol = c("mag", "yr"),crs = 4269, grid = FALSE, map.type = "CartoDB.Positron", layer.name = "Magnitude", legend.opacity = .5, col.regions = hcl.colors(14, palette = list( "Lajolla", "blue")))

framemasterNSAF

```
Both these visualizations show the distribution of points across CSZ and NSAF. The presence of the San Andreas fault is very visible in the second visual. It's easy to see that our data wasn't hugely variable; as our power law distribution showed, a majority of the events were lower magnitude earthquakes- making the predictions from the kNN model more intimidating if they are accurate. 

Something to consider when viewing these visualizations is *human population*. The Quiet Period proved that seismic stations are picking up on human activity and recording it as seismic activity, hence the dramatic decrease in quake events in the year 2020, mainly in CSZ. The area of study across NSAF has a population upwards of 7.753 million. Cities like Portland and Seattle combined have a population of ~1.37 million. It's possible that the high concentration of events we see in NSAF are false positives from seismic stations picking up waves from human activity.

## Conclusions

Takeaways from this exploratory data analysis can be used to understand the behavior between these two faults. Firstly, scatterplots determined a strong relationship between depth and magnitude in both regions that is backed up by literature. Bar charts illustrated patterns in clustering and stress relief on the surface when smaller earthquakes occurred right before a higher magnitude quake. Histograms confirmed the distribution of our data, specifically magnitude, followed a power law distribution, allowing us to move forward with nonparametric methods in our statistical analysis. The Wilcoxon Rank test allowed us to run an analysis mirroring a t-test on non-normal data, resulting in a significant difference in medians between the two groups. Fitting negative binomial and poisson distributions pointed out flaws in the process of predictive modelling with earthquakes, and the resulting percentages saw that the chances of a magnitude 5.5 quake or greater occurring in both regions increased over time, but not necessarily with increases in 'r' in negative binomial distributions. The creating of the kNN algorithm creates a model that can easily be replicated to predict the location of future earthquakes by taking in patterns of depth and magnitude over the 10 year period into account, and resulted in predictions that earthquakes of 'megathrust' magnitude and depth are more likely to occur in Cascadia than San Andreas. Lastly, spatial visualizations show the distribution of earthquakes based on year and magnitude, and show more higher magnitude earthquakes occurring in CSZ. Overall, the results in this analysis back up literature and research that the Cascadia Subduction Zone is prone to a 'megathrust' earthquake. Government and national organizations in the Pacific Northwest must remain aware of the possibility of this and prepare citizens accordingly for what could be a devastating earthquake. 

## References

- Aagaard, Brad T, James Luke Blair, and John Boatwright. “Earthquake Outlook for the San Francisco Bay Region 2014–2043,” n.d., 6.
- Earthquake Glossary. (n.d.). Retrieved May 3, 2022, from https://earthquake.usgs.gov/learn/glossary/
- Geographyrealm. (2016, December 27). What are the Three Plates that Make up the Mendocino Triple Junction? Geography Realm. https://www.geographyrealm.com/three-plates-make-mendocino-triple-junction/
- Ide, Satoshi, Gregory C. Beroza, David R. Shelly, and Takahiko Uchide. “A Scaling Law for Slow Earthquakes.” Nature 447, no. 7140 (May 2007): 76–79. https://doi.org/10.1038/nature05780.
- Plourde, A. P., M. G. Bostock, P. Audet, and A. M. Thomas (2015), Low-frequency earthquakes at the southern Cascadia margin, Geophys. Res. Lett., 42, 4849–4855, doi:10.1002/2015GL064363.
- Power law. (2022). In Wikipedia. https://en.wikipedia.org/w/index.php?title=Power_law&oldid=1084603641
-Quasi-periodic recurrence of large earthquakes on the southern San Andreas fault https://doi-org.prox.lib.ncsu.edu/10.1130/G30746.1
- The Earthquake That Will Devastate the Pacific Northwest | The New Yorker. (n.d.). Retrieved January 12, 2022, from https://www.newyorker.com/magazine/2015/07/20/the-really-big-one
- USGS Science Data Catalog. (n.d.). Retrieved May 3, 2022, from https://data.usgs.gov/datacatalog/
- Wallace, L. M., & Saffer, D. M. (2016). Characteristics and habitat of deep vs. Shallow slow slip events. 2016, T11F-03.
- Witze, A. (2019). Two of the biggest US earthquake faults might be linked. Nature, 576(7786), 191–192. https://doi.org/10.1038/d41586-019-03769-w
 




